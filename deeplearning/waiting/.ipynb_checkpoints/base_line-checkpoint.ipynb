{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length : 2054\n",
      "test length : 734\n",
      "valid length : 881\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "import torch.optim as optim\n",
    "from konlpy.tag import *\n",
    "\n",
    "tagger = Komoran()\n",
    "tokenize = tagger.morphs\n",
    "\n",
    "torch.manual_seed(0)\n",
    "REVIEW = Field(sequential=True,\n",
    "               tokenize=tokenize,\n",
    "               use_vocab=True,\n",
    "               include_lengths=True,\n",
    "               batch_first=True)\n",
    "\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "                                         path='./',\n",
    "                                         train='train.tsv',\n",
    "                                         test='test.tsv', format='tsv',\n",
    "                                         fields=[('review', REVIEW), ('label', LABEL)]\n",
    "\n",
    "                                         )\n",
    "\n",
    "\n",
    "\n",
    "train, valid = train.split(random_state=random.seed(0))\n",
    "print(\"train length : {}\".format(len(train)))\n",
    "print(\"test length : {}\".format(len(test)))\n",
    "print(\"valid length : {}\".format(len(valid)))\n",
    "\n",
    "\n",
    "REVIEW.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch_size = 32\n",
    "REVIEW.build_vocab(train)\n",
    "len(REVIEW.vocab)\n",
    "\n",
    "# Make iterator for splits\n",
    "train_iter, test_iter, val_iter = BucketIterator.splits(\n",
    "    (train, test, valid), batch_size=batch_size, device=device, # device -1 : cpu, device 0 : 남는 gpu\n",
    "    sort_key=lambda x: len(x.review), sort_within_batch=True, repeat=False) # x.TEXT 길이 기준으로 정렬\n",
    "\n",
    "# <center>3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, batch_first=True)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output)\n",
    "\n",
    "        hidden = self.dropout(\n",
    "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(REVIEW.vocab)\n",
    "EMBEDDING_DIM = 70\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "\n",
    "PAD_IDX = REVIEW.vocab.stoi[REVIEW.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(INPUT_DIM,\n",
    "                   EMBEDDING_DIM,\n",
    "                   HIDDEN_DIM,\n",
    "                   OUTPUT_DIM,\n",
    "                   N_LAYERS,\n",
    "                   BIDIRECTIONAL,\n",
    "                   DROPOUT,\n",
    "                   PAD_IDX)\n",
    "model.to(device)\n",
    "\n",
    "import numpy as np\n",
    "# numpy float 출력옵션 변경\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "STEP = 50\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "best_valid_loss =9999999\n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/50] val_loss: 0.8977\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[2/50] val_loss: 0.5335\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[3/50] val_loss: 0.4959\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[4/50] val_loss: 0.4900\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[5/50] val_loss: 0.4764\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[6/50] val_loss: 1.0130\n",
      "\n",
      "\n",
      "[7/50] val_loss: 0.5033\n",
      "\n",
      "\n",
      "[8/50] val_loss: 0.8399\n",
      "\n",
      "\n",
      "[9/50] val_loss: 0.4762\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[10/50] val_loss: 0.4785\n",
      "\n",
      "\n",
      "[11/50] val_loss: 0.4371\n",
      "\n",
      "save model\n",
      "\n",
      "\n",
      "[12/50] val_loss: 0.4670\n",
      "\n",
      "\n",
      "[13/50] val_loss: 1.1172\n",
      "\n",
      "\n",
      "[14/50] val_loss: 0.4614\n",
      "\n",
      "\n",
      "[15/50] val_loss: 0.4784\n",
      "\n",
      "\n",
      "[16/50] val_loss: 0.4429\n",
      "\n",
      "\n",
      "[17/50] val_loss: 0.5288\n",
      "\n",
      "\n",
      "[18/50] val_loss: 0.4885\n",
      "\n",
      "\n",
      "[19/50] val_loss: 0.5676\n",
      "\n",
      "\n",
      "[20/50] val_loss: 0.4585\n",
      "\n",
      "\n",
      "[21/50] val_loss: 0.5371\n",
      "\n",
      "\n",
      "[22/50] val_loss: 0.9924\n",
      "\n",
      "\n",
      "[23/50] val_loss: 0.4948\n",
      "\n",
      "\n",
      "[24/50] val_loss: 0.5683\n",
      "\n",
      "\n",
      "[25/50] val_loss: 0.8437\n",
      "\n",
      "\n",
      "[26/50] val_loss: 0.5667\n",
      "\n",
      "\n",
      "[27/50] val_loss: 0.5149\n",
      "\n",
      "\n",
      "[28/50] val_loss: 0.5938\n",
      "\n",
      "\n",
      "[29/50] val_loss: 0.5429\n",
      "\n",
      "\n",
      "[30/50] val_loss: 0.5424\n",
      "\n",
      "\n",
      "[31/50] val_loss: 0.5870\n",
      "\n",
      "\n",
      "[32/50] val_loss: 0.5964\n",
      "\n",
      "\n",
      "[33/50] val_loss: 0.7695\n",
      "\n",
      "\n",
      "[34/50] val_loss: 0.6454\n",
      "\n",
      "\n",
      "[35/50] val_loss: 0.6427\n",
      "\n",
      "\n",
      "[36/50] val_loss: 0.7813\n",
      "\n",
      "\n",
      "[37/50] val_loss: 0.6160\n",
      "\n",
      "\n",
      "[38/50] val_loss: 0.6104\n",
      "\n",
      "\n",
      "[39/50] val_loss: 0.6805\n",
      "\n",
      "\n",
      "[40/50] val_loss: 0.6903\n",
      "\n",
      "\n",
      "[41/50] val_loss: 0.8438\n",
      "\n",
      "\n",
      "[42/50] val_loss: 0.7044\n",
      "\n",
      "\n",
      "[43/50] val_loss: 0.6957\n",
      "\n",
      "\n",
      "[44/50] val_loss: 0.6552\n",
      "\n",
      "\n",
      "[45/50] val_loss: 0.6822\n",
      "\n",
      "\n",
      "[46/50] val_loss: 0.6999\n",
      "\n",
      "\n",
      "[47/50] val_loss: 0.9243\n",
      "\n",
      "\n",
      "[48/50] val_loss: 0.7823\n",
      "\n",
      "\n",
      "[49/50] val_loss: 0.7247\n",
      "\n",
      "\n",
      "[50/50] val_loss: 0.7912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        model.train()\n",
    "        inputs, lengths = batch.review\n",
    "        targets = batch.label   \n",
    "        model.zero_grad()\n",
    "\n",
    "        preds = model(inputs, lengths).squeeze(1)\n",
    "\n",
    "\n",
    "        loss = loss_function(preds, targets.long()) \n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        val_losses=[]\n",
    "        val_accu = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            inputs, lengths = batch.review\n",
    "            targets = batch.label\n",
    "            preds = model(inputs, lengths).squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "            val_loss = loss_function(preds, targets.long())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            optimizer.step()\n",
    "        print()\n",
    "        string = '[{}/{}] val_loss: {:.4f}'.format(step+1, STEP, np.mean(val_losses))\n",
    "        print(string)\n",
    "\n",
    "        print()\n",
    "\n",
    "        if np.mean(val_losses) < best_valid_loss:\n",
    "            best_valid_loss = np.mean(val_losses)\n",
    "            print(\"save model\")\n",
    "            print()\n",
    "\n",
    "            torch.save(model.state_dict(), 'model_base.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_base.pt'),strict=False)\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8065395095367848\n",
      "loss :  0.48776885618766147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpu/.local/lib/python3.5/site-packages/ipykernel_launcher.py:41: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    }
   ],
   "source": [
    "y_hat = []\n",
    "y_real = []\n",
    "\n",
    "num_equal=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    num_equal = 0\n",
    "    val_loss = 0\n",
    "    for i, batch in enumerate(test_iter):\n",
    "\n",
    "        inputs, lengths = batch.review\n",
    "        targets = batch.label\n",
    "        \n",
    "        if 0 in lengths:\n",
    "            idxes = torch.arange(inputs.size(0))\n",
    "            if USE_CUDA:\n",
    "                idxes = idxes.cuda()\n",
    "            mask = idxes[lengths.ne(0)].long()\n",
    "\n",
    "            inputs = inputs.index_select(0, mask)\n",
    "            lengths = lengths.masked_select(lengths.ne(0))\n",
    "            targets = targets.index_select(0, mask)\n",
    "\n",
    "\n",
    "        preds = model(inputs, lengths)\n",
    "        loss = loss_function(preds, targets) \n",
    "\n",
    "\n",
    "        acc = categorical_accuracy(preds, targets)\n",
    "\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True).squeeze(0) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(targets) # 같은것만 찾는 코드\n",
    "\n",
    "        max_preds = max_preds.squeeze()\n",
    "        y_hat.append(max_preds.tolist())\n",
    "        y_real.append(targets.tolist())\n",
    "        num_equal += int(torch.eq(max_preds, targets).sum())\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "print(\"Accuracy : \" , num_equal / len(pd.DataFrame.from_csv('test.tsv', sep='\\t', header=None)))\n",
    "print(\"loss : \", val_loss/len(test_iter))\n",
    "\n",
    "\n",
    "y_hat_flat = list(itertools.chain(*y_hat))\n",
    "y_real_flat = list(itertools.chain(*y_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 67  88]\n",
      " [ 54 525]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      class 1       0.55      0.43      0.49       155\n",
      "class missing       0.86      0.91      0.88       579\n",
      "\n",
      "    micro avg       0.81      0.81      0.81       734\n",
      "    macro avg       0.71      0.67      0.68       734\n",
      " weighted avg       0.79      0.81      0.80       734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "print(confusion_matrix(y_real_flat,y_hat_flat))\n",
    "\n",
    "print(classification_report(y_real_flat, y_hat_flat, target_names=['class 1','class missing' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = classification_report(y_real_flat, y_hat_flat, target_names=['class 1','class missing' ], output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Classifier(\n",
       "  (embedding): Embedding(9218, 70, padding_idx=1)\n",
       "  (rnn): LSTM(70, 128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3)\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = result_dic['class 1']['f1-score'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48550724637681164"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('logging/{}_base.json'.format(err), 'w', encoding='utf-8') as make_file:\n",
    "    json.dump(result_dic, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
