{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "import torch.optim as optim\n",
    "from konlpy.tag import *\n",
    "\n",
    "tagger = Komoran()\n",
    "tokenize = tagger.morphs\n",
    "\n",
    "torch.manual_seed(0)\n",
    "REVIEW = Field(sequential=True,\n",
    "               tokenize=tokenize,\n",
    "               use_vocab=True,\n",
    "               include_lengths=True,\n",
    "               batch_first=True)\n",
    "\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, test = data.TabularDataset.splits(\n",
    "                                         path='./',\n",
    "                                         train='train.tsv',\n",
    "                                         test='test.tsv', format='tsv',\n",
    "                                         fields=[('review', REVIEW), ('label', LABEL)]\n",
    "\n",
    "                                         )\n",
    "\n",
    "\n",
    "\n",
    "train, valid = train.split(random_state=random.seed(0))\n",
    "print(\"train length : {}\".format(len(train)))\n",
    "print(\"test length : {}\".format(len(test)))\n",
    "print(\"valid length : {}\".format(len(valid)))\n",
    "\n",
    "\n",
    "REVIEW.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch_size = 128\n",
    "REVIEW.build_vocab(train)\n",
    "len(REVIEW.vocab)\n",
    "\n",
    "# Make iterator for splits\n",
    "train_iter, test_iter, val_iter = BucketIterator.splits(\n",
    "    (train, test, valid), batch_size=batch_size, device=device, # device -1 : cpu, device 0 : 남는 gpu\n",
    "    sort_key=lambda x: len(x.review), sort_within_batch=True, repeat=False) # x.TEXT 길이 기준으로 정렬\n",
    "\n",
    "# <center>3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, text_lengths, batch_first=True)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output)\n",
    "\n",
    "        hidden = self.dropout(\n",
    "            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(REVIEW.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "N_LAYERS = 4\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.3\n",
    "\n",
    "PAD_IDX = REVIEW.vocab.stoi[REVIEW.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(INPUT_DIM,\n",
    "                   EMBEDDING_DIM,\n",
    "                   HIDDEN_DIM,\n",
    "                   OUTPUT_DIM,\n",
    "                   N_LAYERS,\n",
    "                   BIDIRECTIONAL,\n",
    "                   DROPOUT,\n",
    "                   PAD_IDX)\n",
    "model.to(device)\n",
    "\n",
    "import numpy as np\n",
    "# numpy float 출력옵션 변경\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "STEP = 50\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True)\n",
    "\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "best_valid_loss =9999999\n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        model.train()\n",
    "        inputs, lengths = batch.review\n",
    "        targets = batch.label   \n",
    "        model.zero_grad()\n",
    "\n",
    "        preds = model(inputs, lengths).squeeze(1)\n",
    "\n",
    "\n",
    "        loss = loss_function(preds, targets.long()) \n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        val_losses=[]\n",
    "        val_accu = []\n",
    "        for i, batch in enumerate(val_iter):\n",
    "            inputs, lengths = batch.review\n",
    "            targets = batch.label\n",
    "            preds = model(inputs, lengths).squeeze(1)\n",
    "\n",
    "\n",
    "\n",
    "            val_loss = loss_function(preds, targets.long())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            optimizer.step()\n",
    "        print()\n",
    "        string = '[{}/{}] val_loss: {:.4f}'.format(step+1, STEP, np.mean(val_losses))\n",
    "        print(string)\n",
    "\n",
    "        print()\n",
    "\n",
    "        if np.mean(val_losses) < best_valid_loss:\n",
    "            best_valid_loss = np.mean(val_losses)\n",
    "            print(\"save model\")\n",
    "            print()\n",
    "\n",
    "            torch.save(model.state_dict(), 'model_base.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_base.pt'),strict=False)\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = []\n",
    "y_real = []\n",
    "\n",
    "num_equal=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    num_equal = 0\n",
    "    val_loss = 0\n",
    "    for i, batch in enumerate(test_iter):\n",
    "\n",
    "        inputs, lengths = batch.review\n",
    "        targets = batch.label\n",
    "        \n",
    "        if 0 in lengths:\n",
    "            idxes = torch.arange(inputs.size(0))\n",
    "            if USE_CUDA:\n",
    "                idxes = idxes.cuda()\n",
    "            mask = idxes[lengths.ne(0)].long()\n",
    "\n",
    "            inputs = inputs.index_select(0, mask)\n",
    "            lengths = lengths.masked_select(lengths.ne(0))\n",
    "            targets = targets.index_select(0, mask)\n",
    "\n",
    "\n",
    "        preds = model(inputs, lengths)\n",
    "        loss = loss_function(preds, targets) \n",
    "\n",
    "\n",
    "        acc = categorical_accuracy(preds, targets)\n",
    "\n",
    "        max_preds = preds.argmax(dim = 1, keepdim = True).squeeze(0) # get the index of the max probability\n",
    "        correct = max_preds.squeeze(1).eq(targets) # 같은것만 찾는 코드\n",
    "\n",
    "        max_preds = max_preds.squeeze()\n",
    "        y_hat.append(max_preds.tolist())\n",
    "        y_real.append(targets.tolist())\n",
    "        num_equal += int(torch.eq(max_preds, targets).sum())\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "print(\"Accuracy : \" , num_equal / len(pd.DataFrame.from_csv('test.tsv', sep='\\t', header=None)))\n",
    "print(\"loss : \", val_loss/len(test_iter))\n",
    "\n",
    "\n",
    "y_hat_flat = list(itertools.chain(*y_hat))\n",
    "y_real_flat = list(itertools.chain(*y_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "print(confusion_matrix(y_real_flat,y_hat_flat))\n",
    "\n",
    "print(classification_report(y_real_flat, y_hat_flat, target_names=['class 0', 'class 1','class missing' ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = classification_report(y_real_flat, y_hat_flat, target_names=['class 0', 'class 1','class missing' ], output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = ( result_dic['class 0']['f1-score'] + result_dic['class 1']['f1-score'] ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('logging/{}.json'.format(err), 'w', encoding='utf-8') as make_file:\n",
    "    json.dump(result_dic, make_file, ensure_ascii=False, indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
